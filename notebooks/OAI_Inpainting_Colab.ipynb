{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 OAI X-ray Inpainting - Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/johnreynolds3d/OAI-inpainting/blob/master/notebooks/OAI_Inpainting_Colab.ipynb)\n",
    "\n",
    "**Comprehensive testing and training of 9+ inpainting model variants on OAI knee X-ray data**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What This Notebook Does\n",
    "\n",
    "This notebook tests and trains state-of-the-art image inpainting models on OAI (Osteoarthritis Initiative) knee X-ray data:\n",
    "\n",
    "### 📊 Model Variants Available:\n",
    "- **AOT-GAN**: CelebA-HQ, Places2, OAI-trained\n",
    "- **ICT**: FFHQ, ImageNet, Places2_Nature, OAI-trained  \n",
    "- **RePaint**: CelebA-HQ, ImageNet, Places2\n",
    "\n",
    "**Total: 9 different model variants** tested on your OAI data!\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick Start (5 Simple Steps)\n",
    "\n",
    "### 1️⃣ **Setup Environment**\n",
    "- Run: \"Check GPU availability\"\n",
    "- Run: \"Alternative setup method (if Git fails)\"\n",
    "→ Clones repo, creates symlinks, installs dependencies\n",
    "\n",
    "### 2️⃣ **Mount Google Drive**\n",
    "- Run: \"Mount Google Drive and setup data\"\n",
    "→ Makes your OAI_untracked folder accessible\n",
    "\n",
    "### 3️⃣ **Verify Setup**\n",
    "- Run: \"Verify installation and setup\"\n",
    "→ Quick check that imports work\n",
    "\n",
    "### 4️⃣ **Generate Dataset Splits** (First time only)\n",
    "- Run: \"GENERATE DATASET SPLITS\"\n",
    "→ Creates **perfectly balanced** train/valid/test splits using **ALL 539 images**\n",
    "→ 80/10/10 split with equal low/high BMD representation\n",
    "→ Creates subset_4 (4 test images) for quick testing\n",
    "\n",
    "### 5️⃣ **Test All Models**\n",
    "- Run: \"PIPELINE RUNNER SETUP\" (imports functions)\n",
    "- Run: \"QUICK START - Test ALL Model Variants\"\n",
    "→ Tests all 9 models in ~30-60 minutes\n",
    "\n",
    "**Done!** Results saved to Google Drive automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Data Organization\n",
    "\n",
    "**Recommended setup:**\n",
    "- ✅ **Code**: Cloned from GitHub (always up-to-date)\n",
    "- ✅ **Data**: Stored in Google Drive (persistent across sessions)\n",
    "- ✅ **Results**: Generated in Colab (downloadable as ZIP)\n",
    "\n",
    "**Required data structure:**\n",
    "```\n",
    "OAI_untracked/\n",
    "├── data/\n",
    "│   ├── oai/\n",
    "│   │   ├── img/          # 539 PNG X-ray images\n",
    "│   │   ├── data.csv      # BMD values for each image\n",
    "│   │   └── split.py      # Balanced split generator\n",
    "│   └── pretrained/       # Model checkpoint files\n",
    "│       ├── aot-gan/\n",
    "│       ├── ict/\n",
    "│       └── repaint/\n",
    "```\n",
    "\n",
    "**📊 New Balanced Split (Oct 2025):**\n",
    "- ✅ Uses **ALL 539 images** (previously only ~268)\n",
    "- ✅ **80% train** (431 images), **10% val** (53 images), **10% test** (55 images)\n",
    "- ✅ **Perfectly balanced**: Equal low/high BMD in each split\n",
    "- ✅ **Mutually exclusive**: No image overlap between splits\n",
    "\n",
    "**💾 Results Persistence:**\n",
    "Both `data/` and `results/` are symlinked to `OAI_untracked/` in Google Drive, ensuring:\n",
    "- ✅ Results survive Colab session restarts\n",
    "- ✅ Identical behavior on local machine and Colab\n",
    "- ✅ All outputs automatically saved to Google Drive\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Usage Modes\n",
    "\n",
    "### Mode 1: **Quick Testing** (Recommended for first time)\n",
    "- ⏱️ Time: ~30-60 minutes\n",
    "- 🎯 Purpose: Test all pretrained models on 4 sample images\n",
    "- 📊 Output: Comparison of all 9 model variants\n",
    "- 💻 Run: Cell 11 (Comprehensive Test)\n",
    "\n",
    "### Mode 2: **Full Training** (Advanced)\n",
    "- ⏱️ Time: 6-8 hours\n",
    "- 🎯 Purpose: Train new models on full OAI dataset\n",
    "- 📊 Output: New trained models + evaluation\n",
    "- 💻 Run: Cell 14 (Training Pipeline)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and setup data\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    print(\"✅ Google Drive mounted successfully\")\n",
    "\n",
    "    # Check for OAI data in Google Drive\n",
    "    oai_drive_path = Path(\"/content/drive/MyDrive/Colab Notebooks/OAI_untracked\")\n",
    "    if oai_drive_path.exists():\n",
    "        print(f\"✅ OAI data found in Google Drive: {oai_drive_path}\")\n",
    "\n",
    "        # Create symlinks to Google Drive data\n",
    "        data_dir = Path(\"data\")\n",
    "        if not data_dir.exists():\n",
    "            data_dir.mkdir()\n",
    "\n",
    "        # Link to Google Drive data\n",
    "        oai_link = data_dir / \"oai\"\n",
    "        if not oai_link.exists():\n",
    "            oai_link.symlink_to(oai_drive_path)\n",
    "            print(\"🔗 Created symlink to Google Drive OAI data\")\n",
    "    else:\n",
    "        print(\"❌ OAI data not found in Google Drive\")\n",
    "        print(\n",
    "            \"💡 Please ensure your data is in: /content/drive/MyDrive/Colab Notebooks/OAI_untracked\"\n",
    "        )\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⚠️ Google Colab not detected - skipping Drive mount\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error mounting Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative setup method (if Git fails)\n",
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def setup_repository_alternative():\n",
    "    \"\"\"Alternative method to setup repository without Git\"\"\"\n",
    "    print(\"🔄 Using alternative setup method...\")\n",
    "\n",
    "    # IMPORTANT: Navigate to Google Drive to maintain sibling directory structure\n",
    "    # This ensures relative symlinks work (data -> ../OAI_untracked/data/)\n",
    "    # Making it portable between local and Colab environments\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        colab_notebooks = Path(\"/content/drive/MyDrive/Colab Notebooks\")\n",
    "        if colab_notebooks.exists():\n",
    "            os.chdir(colab_notebooks)\n",
    "            print(f\"✅ Working directory: {Path.cwd()}\")\n",
    "            print(\"💡 Using relative paths for portability\")\n",
    "        else:\n",
    "            print(\"⚠️ Google Drive not mounted or path not found\")\n",
    "            print(f\"📁 Current directory: {Path.cwd()}\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Not in Colab environment - using current directory\")\n",
    "        print(f\"📁 Current directory: {Path.cwd()}\")\n",
    "\n",
    "    if not Path(\"OAI-inpainting\").exists():\n",
    "        print(\"📥 Downloading repository as ZIP...\")\n",
    "        try:\n",
    "            # Download the repository as ZIP\n",
    "            zip_url = \"https://github.com/johnreynolds3d/OAI-inpainting/archive/refs/heads/master.zip\"\n",
    "            zip_path = \"OAI-inpainting-master.zip\"\n",
    "\n",
    "            print(f\"📥 Downloading from: {zip_url}\")\n",
    "            urllib.request.urlretrieve(zip_url, zip_path)\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            print(\"📦 Extracting repository...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(\".\")\n",
    "\n",
    "            # Rename the extracted folder\n",
    "            if Path(\"OAI-inpainting-master\").exists():\n",
    "                if Path(\"OAI-inpainting\").exists():\n",
    "                    import shutil\n",
    "\n",
    "                    shutil.rmtree(\"OAI-inpainting\")\n",
    "                Path(\"OAI-inpainting-master\").rename(\"OAI-inpainting\")\n",
    "\n",
    "            # Clean up ZIP file\n",
    "            Path(zip_path).unlink()\n",
    "            print(\"✅ Repository downloaded and extracted successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Download failed: {e}\")\n",
    "            print(\"💡 Please check your internet connection and try again\")\n",
    "            return False\n",
    "\n",
    "    # Change to project directory\n",
    "    os.chdir(\"OAI-inpainting\")\n",
    "    print(f\"📂 Current directory: {Path.cwd()}\")\n",
    "\n",
    "    # Create relative symlinks (portable between local and Colab)\n",
    "    print(\"\\n🔗 Setting up portable symlinks...\")\n",
    "\n",
    "    # Remove any broken files that might have been cloned from Git\n",
    "    for name in [\"data\", \"results\"]:\n",
    "        path = Path(name)\n",
    "        if path.is_file():  # Broken symlink tracked as file\n",
    "            path.unlink()\n",
    "            print(f\"🗑️  Removed broken {name} file\")\n",
    "\n",
    "    # Create data and results symlinks using relative paths (portable!)\n",
    "    data_dir = Path(\"data\")\n",
    "    results_dir = Path(\"results\")\n",
    "    relative_data_path = Path(\"../OAI_untracked/data\")\n",
    "    relative_results_path = Path(\"../OAI_untracked/results\")\n",
    "\n",
    "    # Setup data symlink\n",
    "    if not data_dir.exists():\n",
    "        if relative_data_path.exists():\n",
    "            data_dir.symlink_to(relative_data_path)\n",
    "            print(\"✅ Created data symlink -> ../OAI_untracked/data/ (relative)\")\n",
    "        else:\n",
    "            print(\"⚠️ OAI_untracked/data not found as sibling directory\")\n",
    "            print(\n",
    "                \"💡 Expected structure: parent_dir/OAI-inpainting/ and parent_dir/OAI_untracked/\"\n",
    "            )\n",
    "    elif data_dir.is_symlink():\n",
    "        print(\"✅ Data symlink already exists (relative)\")\n",
    "\n",
    "    # Setup results symlink (same as local - results persist in Google Drive!)\n",
    "    if not results_dir.exists():\n",
    "        # Ensure parent results directory exists\n",
    "        if not relative_results_path.exists():\n",
    "            try:\n",
    "                relative_results_path.mkdir(parents=True, exist_ok=True)\n",
    "                print(\"📁 Created results directory in OAI_untracked\")\n",
    "            except:\n",
    "                pass  # May not have permissions, will fallback\n",
    "        \n",
    "        # Try to create symlink\n",
    "        if relative_results_path.exists():\n",
    "            results_dir.symlink_to(relative_results_path)\n",
    "            print(\"✅ Created results symlink -> ../OAI_untracked/results/ (relative)\")\n",
    "            print(\"💾 Results will persist in Google Drive!\")\n",
    "        else:\n",
    "            # Fallback: create local directory\n",
    "            results_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"⚠️  Results created as local directory (won't persist in Drive)\")\n",
    "    elif results_dir.is_symlink():\n",
    "        print(\"✅ Results symlink already exists (relative)\")\n",
    "    else:\n",
    "        print(\"⚠️  Results exists as real directory (won't persist in Drive)\")\n",
    "        print(\"💡 Delete it to create symlink for Google Drive persistence\")\n",
    "\n",
    "    # Install core dependencies manually\n",
    "    print(\"\\n📦 Installing core dependencies...\")\n",
    "    core_deps = [\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"numpy\",\n",
    "        \"opencv-python\",\n",
    "        \"pillow\",\n",
    "        \"scikit-image\",\n",
    "        \"scipy\",\n",
    "        \"pandas\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"pyyaml\",\n",
    "        \"tqdm\",\n",
    "        \"tensorboard\",\n",
    "        \"wandb\",\n",
    "        \"scikit-learn\",\n",
    "    ]\n",
    "\n",
    "    for dep in core_deps:\n",
    "        try:\n",
    "            subprocess.run([\"pip\", \"install\", dep], check=True, capture_output=True)\n",
    "            print(f\"✅ Installed {dep}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️ Failed to install {dep}\")\n",
    "\n",
    "    print(\"\\n✅ Alternative setup complete!\")\n",
    "    print(f\"📁 Project location: {Path.cwd()}\")\n",
    "    print(\n",
    "        f\"📁 Data location: {Path('data').resolve() if Path('data').exists() else 'Not linked'}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"📁 Results location: {Path('results').resolve() if Path('results').exists() else 'Not linked'}\"\n",
    "    )\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run alternative setup\n",
    "setup_repository_alternative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Troubleshooting\n",
    "\n",
    "If you encounter issues with the setup:\n",
    "\n",
    "1. **Git not available**: The notebook will automatically fall back to downloading the repository as a ZIP file\n",
    "2. **Dependency installation fails**: Core dependencies will be installed individually\n",
    "3. **Permission errors**: Restart the runtime and try again\n",
    "4. **Import errors**: Make sure you're in the correct directory (`OAI-inpainting`)\n",
    "5. **NotADirectoryError**: Run the \"FIX: Results Directory Issue\" cell below\n",
    "\n",
    "**Quick fixes:**\n",
    "- **Fix results directory**: Run the next cell (Results Directory Fix)\n",
    "- Restart runtime: `Runtime → Restart runtime`\n",
    "- Check directory: `!pwd`\n",
    "- List files: `!ls -la`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 FIX: Results Directory Issue\n",
    "# Run this cell if you get \"NotADirectoryError\" when running tests\n",
    "# This fixes the results directory by converting it from a symlink to a real directory\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"🔧 Fixing results directory...\")\n",
    "\n",
    "# Try to navigate to project directory\n",
    "try:\n",
    "    project_dirs = [\n",
    "        Path(\"/content/drive/MyDrive/Colab Notebooks/OAI-inpainting\"),\n",
    "        Path(\"/content/OAI-inpainting\"),\n",
    "        Path(\"OAI-inpainting\"),\n",
    "        Path.cwd(),\n",
    "    ]\n",
    "\n",
    "    for project_dir in project_dirs:\n",
    "        if project_dir.exists() and (project_dir / \"models\").exists():\n",
    "            os.chdir(project_dir)\n",
    "            print(f\"📁 Working directory: {os.getcwd()}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"⚠️ Could not find OAI-inpainting directory\")\n",
    "        print(f\"📁 Current directory: {os.getcwd()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error navigating: {e}\")\n",
    "    print(f\"📁 Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Check and fix results directory\n",
    "results = Path(\"results\")\n",
    "\n",
    "if results.is_symlink():\n",
    "    target = os.readlink(results)\n",
    "    print(f\"⚠️ Found symlink: results -> {target}\")\n",
    "    results.unlink()\n",
    "    print(\"🗑️ Removed symlink\")\n",
    "elif results.is_file():\n",
    "    print(\"⚠️ Found file instead of directory\")\n",
    "    results.unlink()\n",
    "    print(\"🗑️ Removed file\")\n",
    "elif results.is_dir():\n",
    "    print(\"✅ Results is already a real directory\")\n",
    "else:\n",
    "    print(\"ℹ️ Results doesn't exist yet\")\n",
    "\n",
    "# Create as real directory\n",
    "try:\n",
    "    results.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✅ Created real directory: {results.resolve()}\")\n",
    "\n",
    "    # Verify\n",
    "    if results.is_dir() and not results.is_symlink():\n",
    "        print(\"\\n🎉 SUCCESS! Results is now a proper directory\")\n",
    "        print(\"📂 You can now run the comprehensive test!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Results exists but may have issues\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error creating directory: {e}\")\n",
    "    print(\"💡 Try: Runtime → Restart runtime, then re-run Cell 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and setup\n",
    "try:\n",
    "    from src.paths import get_project_root\n",
    "\n",
    "    print(\"✅ Core modules imported successfully\")\n",
    "    print(f\"📁 Project root: {get_project_root()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "\n",
    "# Check available models\n",
    "models_dir = Path(\"models\")\n",
    "if models_dir.exists():\n",
    "    print(\"\\n📋 Available models:\")\n",
    "    for model in models_dir.iterdir():\n",
    "        if model.is_dir():\n",
    "            print(f\"  - {model.name}\")\n",
    "else:\n",
    "    print(\"❌ Models directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data from Google Drive\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"📊 Setting up data from Google Drive...\")\n",
    "\n",
    "# Define paths\n",
    "drive_data_path = Path(\"/content/drive/MyDrive/Colab Notebooks/OAI_untracked\")\n",
    "local_data_path = Path(\"data\")\n",
    "\n",
    "# Check if Google Drive data exists\n",
    "if drive_data_path.exists():\n",
    "    print(f\"✅ Found OAI data in Google Drive: {drive_data_path}\")\n",
    "\n",
    "    # Check data structure\n",
    "    oai_img_path = drive_data_path / \"data\" / \"oai\" / \"img\"\n",
    "    pretrained_path = drive_data_path / \"data\" / \"pretrained\"\n",
    "\n",
    "    if oai_img_path.exists():\n",
    "        img_count = len(list(oai_img_path.glob(\"*.png\")))\n",
    "        print(f\"📸 Found {img_count} OAI X-ray images\")\n",
    "    else:\n",
    "        print(\"⚠️ OAI images not found in expected location\")\n",
    "\n",
    "    if pretrained_path.exists():\n",
    "        model_files = list(pretrained_path.rglob(\"*.pt\")) + list(\n",
    "            pretrained_path.rglob(\"*.pth\")\n",
    "        )\n",
    "        print(f\"🤖 Found {len(model_files)} pretrained model files\")\n",
    "    else:\n",
    "        print(\"⚠️ Pretrained models not found\")\n",
    "\n",
    "    # Check if data is already linked or setup\n",
    "    if local_data_path.is_symlink():\n",
    "        print(\"\\n✅ Data already linked via symlink (from Cell 3)\")\n",
    "        print(f\"   {local_data_path} → {os.readlink(local_data_path)}\")\n",
    "    elif (local_data_path / \"oai\").is_symlink():\n",
    "        print(\"\\n✅ Data already linked via symlink (from Cell 3)\")\n",
    "        print(f\"   {local_data_path / 'oai'} → {os.readlink(local_data_path / 'oai')}\")\n",
    "    else:\n",
    "        # Try to copy data for better performance\n",
    "        print(\"\\n📋 Attempting to copy data from Google Drive to local storage...\")\n",
    "        print(\"   (This improves performance but requires disk space)\")\n",
    "\n",
    "        # Copy only if not already copied (check for a marker file)\n",
    "        marker_file = local_data_path / \".data_copied\"\n",
    "        if not marker_file.exists():\n",
    "            try:\n",
    "                # Create local data directory\n",
    "                local_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Copy the entire data directory structure\n",
    "                source_data_path = drive_data_path / \"data\"\n",
    "                if source_data_path.exists():\n",
    "                    # Use rsync-like approach with os.walk for better control\n",
    "                    print(\"   Copying files... (this may take a few minutes)\")\n",
    "                    file_count = 0\n",
    "                    for root, _dirs, files in os.walk(source_data_path):\n",
    "                        rel_path = Path(root).relative_to(source_data_path)\n",
    "                        dest_dir = local_data_path / rel_path\n",
    "                        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                        for file in files:\n",
    "                            src_file = Path(root) / file\n",
    "                            dst_file = dest_dir / file\n",
    "                            if not dst_file.exists():\n",
    "                                shutil.copy2(src_file, dst_file)\n",
    "                                file_count += 1\n",
    "                                if file_count % 100 == 0:\n",
    "                                    print(f\"   Copied {file_count} files...\")\n",
    "\n",
    "                    print(f\"✅ Data copied successfully ({file_count} files)\")\n",
    "                    marker_file.touch()\n",
    "                else:\n",
    "                    print(\"❌ Data directory not found in expected structure\")\n",
    "                    print(\"💡 Please check your Google Drive structure\")\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)[:200]  # Truncate to prevent massive output\n",
    "                print(f\"❌ Error copying data: {error_msg}...\")\n",
    "                print(\"💡 Falling back to symlink approach...\")\n",
    "                # Fallback to symlink\n",
    "                try:\n",
    "                    if local_data_path.exists() and not local_data_path.is_symlink():\n",
    "                        shutil.rmtree(local_data_path)\n",
    "                    if not local_data_path.exists():\n",
    "                        local_data_path.symlink_to(drive_data_path / \"data\")\n",
    "                        print(\"✅ Created symlink instead\")\n",
    "                except Exception as link_error:\n",
    "                    print(f\"❌ Symlink also failed: {link_error}\")\n",
    "        else:\n",
    "            print(\"✅ Data already copied (skipping)\")\n",
    "\n",
    "    # Verify data structure\n",
    "    print(\"\\n📋 Verifying data structure...\")\n",
    "    oai_local_path = local_data_path / \"oai\"\n",
    "    pretrained_local_path = local_data_path / \"pretrained\"\n",
    "\n",
    "    # Check OAI data\n",
    "    if oai_local_path.exists():\n",
    "        img_path = oai_local_path / \"img\"\n",
    "        if img_path.exists():\n",
    "            img_count = len(list(img_path.glob(\"*.png\")))\n",
    "            print(f\"  ✅ oai/img/ ({img_count} PNG files)\")\n",
    "        else:\n",
    "            print(\"  ❌ oai/img/ (missing)\")\n",
    "    else:\n",
    "        print(\"  ❌ oai/ directory (missing)\")\n",
    "\n",
    "    # Check pretrained models\n",
    "    if pretrained_local_path.exists():\n",
    "        model_files = list(pretrained_local_path.rglob(\"*.pt\")) + list(\n",
    "            pretrained_local_path.rglob(\"*.pth\")\n",
    "        )\n",
    "        print(f\"  ✅ pretrained/ ({len(model_files)} model files)\")\n",
    "\n",
    "        # Check specific model directories\n",
    "        for model_dir in [\"aot-gan\", \"ict\", \"repaint\"]:\n",
    "            model_path = pretrained_local_path / model_dir\n",
    "            if model_path.exists():\n",
    "                model_count = len(\n",
    "                    list(model_path.rglob(\"*.pt\")) + list(model_path.rglob(\"*.pth\"))\n",
    "                )\n",
    "                print(f\"    ✅ {model_dir}/ ({model_count} files)\")\n",
    "            else:\n",
    "                print(f\"    ❌ {model_dir}/ (missing)\")\n",
    "    else:\n",
    "        print(\"  ❌ pretrained/ directory (missing)\")\n",
    "\n",
    "    # Check for generated directories (will be created by split.py)\n",
    "    generated_dirs = [\"train\", \"valid\", \"test\"]\n",
    "    for dir_name in generated_dirs:\n",
    "        dir_path = oai_local_path / dir_name\n",
    "        if dir_path.exists():\n",
    "            file_count = len(list(dir_path.rglob(\"*\")))\n",
    "            print(f\"  ✅ oai/{dir_name}/ ({file_count} files) - Generated\")\n",
    "        else:\n",
    "            print(f\"  ⏳ oai/{dir_name}/ (will be generated by split.py)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ OAI data not found in Google Drive\")\n",
    "    print(\"💡 Please ensure your data is uploaded to:\")\n",
    "    print(\"   /content/drive/MyDrive/Colab Notebooks/OAI_untracked\")\n",
    "    print(\"\\n📤 Expected structure:\")\n",
    "    print(\"   OAI_untracked/\")\n",
    "    print(\"   ├── data/\")\n",
    "    print(\"   │   ├── oai/\")\n",
    "    print(\"   │   │   └── img/          # 539 PNG files\")\n",
    "    print(\"   │   └── pretrained/       # Model files\")\n",
    "    print(\"   └── README.md\")\n",
    "    print(\"\\n📤 To upload data:\")\n",
    "    print(\"1. Go to Google Drive\")\n",
    "    print(\"2. Navigate to 'Colab Notebooks' folder\")\n",
    "    print(\"3. Create 'OAI_untracked' folder\")\n",
    "    print(\"4. Upload your OAI dataset files there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Update Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 GENERATE DATASET SPLITS (Required before testing!)\n",
    "# This creates PERFECTLY BALANCED train/valid/test splits using ALL 539 images\n",
    "# • 80% train (431 images), 10% validation (53 images), 10% test (55 images)\n",
    "# • Equal low/high BMD representation in each split\n",
    "# • Mutually exclusive (no image overlap between splits)\n",
    "# • Also creates subset_4 (4 test images) for quick testing\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🔄 GENERATING PERFECTLY BALANCED DATASET SPLITS\")\n",
    "print(\"Using ALL 539 images with 80/10/10 split\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if splits already exist\n",
    "subset_4_path = Path(\"data/oai/test/img/subset_4\")\n",
    "if subset_4_path.exists() and any(subset_4_path.glob(\"*.png\")):\n",
    "    file_count = len(list(subset_4_path.glob(\"*.png\")))\n",
    "    print(\"✅ Splits already exist!\")\n",
    "    print(f\"✅ Found {file_count} files in subset_4\")\n",
    "    print(\"\\n💡 Skipping split generation - already done\")\n",
    "else:\n",
    "    print(\"⚠️  Splits not found - generating now...\")\n",
    "    print(\"This will create:\")\n",
    "    print(\"  • Train/valid/test splits\")\n",
    "    print(\"  • Random masks\")\n",
    "    print(\"  • Edge maps\")\n",
    "    print(\"  • Inverted masks\")\n",
    "    print(\"  • subset_4 evaluation set (4 images)\")\n",
    "\n",
    "    # Run split.py\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"split.py\"],\n",
    "        check=False,\n",
    "        cwd=\"data/oai\",\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=300,\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✅ PERFECTLY BALANCED dataset splits generated successfully!\")\n",
    "        print(\"\\n📊 Split Summary:\")\n",
    "\n",
    "        # Count actual files in each split\n",
    "        train_count = (\n",
    "            len(list(Path(\"data/oai/train/img\").glob(\"*.png\")))\n",
    "            if Path(\"data/oai/train/img\").exists()\n",
    "            else 0\n",
    "        )\n",
    "        val_count = (\n",
    "            len(list(Path(\"data/oai/valid/img\").glob(\"*.png\")))\n",
    "            if Path(\"data/oai/valid/img\").exists()\n",
    "            else 0\n",
    "        )\n",
    "        test_count = (\n",
    "            len(list(Path(\"data/oai/test/img\").glob(\"*.png\")))\n",
    "            if Path(\"data/oai/test/img\").exists()\n",
    "            else 0\n",
    "        )\n",
    "        total_count = train_count + val_count + test_count\n",
    "\n",
    "        print(\n",
    "            f\"  📁 Train: {train_count} images ({train_count / total_count * 100:.1f}%)\"\n",
    "        )\n",
    "        print(f\"  📁 Valid: {val_count} images ({val_count / total_count * 100:.1f}%)\")\n",
    "        print(\n",
    "            f\"  📁 Test:  {test_count} images ({test_count / total_count * 100:.1f}%)\"\n",
    "        )\n",
    "        print(f\"  📁 TOTAL: {total_count} images\")\n",
    "\n",
    "        # Verify subset_4 was created\n",
    "        if subset_4_path.exists():\n",
    "            img_count = len(list(subset_4_path.glob(\"*.png\")))\n",
    "            print(\n",
    "                f\"\\n✅ subset_4 created with {img_count} images (2 low BMD + 2 high BMD)\"\n",
    "            )\n",
    "\n",
    "            # Verify all required directories\n",
    "            required_dirs = [\n",
    "                \"data/oai/test/img/subset_4\",\n",
    "                \"data/oai/test/mask/subset_4\",\n",
    "                \"data/oai/test/edge/subset_4\",\n",
    "                \"data/oai/test/mask_inv/subset_4\",\n",
    "            ]\n",
    "\n",
    "            print(\"\\n📋 Verification:\")\n",
    "            for dir_path in required_dirs:\n",
    "                p = Path(dir_path)\n",
    "                if p.exists():\n",
    "                    count = len(list(p.glob(\"*.png\")))\n",
    "                    print(\n",
    "                        f\"  ✅ {dir_path.split('/')[-2]}/{dir_path.split('/')[-1]}/: {count} files\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"  ❌ {dir_path}: missing\")\n",
    "\n",
    "            print(\"\\n🎉 Ready for training, testing, and evaluation!\")\n",
    "        else:\n",
    "            print(\"⚠️ subset_4 not created - check output above\")\n",
    "            if result.stdout:\n",
    "                print(f\"Output: {result.stdout}\")\n",
    "    else:\n",
    "        print(f\"❌ Split failed with exit code {result.returncode}\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        print(\"\\n💡 Try running manually:\")\n",
    "        print(\"  !cd data/oai && python split.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Training, Testing & Evaluation\n",
    "\n",
    "Now that you have the perfectly balanced dataset splits, you can:\n",
    "- **Test** pretrained models on the test set\n",
    "- **Train** new models on the full 431-image training set\n",
    "- **Evaluate** model performance with comprehensive metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 PIPELINE RUNNER SETUP\n",
    "# Import the pipeline runner functions\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"scripts\")\n",
    "\n",
    "try:\n",
    "    from colab_comprehensive_test import ModelTester\n",
    "    from colab_pipeline import run_full_pipeline, run_phase\n",
    "\n",
    "    print(\"✅ Pipeline runner imported successfully!\")\n",
    "    print(\"\")\n",
    "    print(\"🎯 Available functions:\")\n",
    "    print(\"\")\n",
    "    print(\"📊 COMPREHENSIVE TESTING (Recommended):\")\n",
    "    print(\"  - ModelTester().run_comprehensive_test() - Test ALL 9 model variants\")\n",
    "    print(\"     • AOT-GAN: CelebA-HQ, Places2, OAI\")\n",
    "    print(\"     • ICT: FFHQ, ImageNet, Places2_Nature, OAI\")\n",
    "    print(\"     • RePaint: CelebA-HQ, ImageNet, Places2\")\n",
    "    print(\"\")\n",
    "    print(\"🔄 PHASED PIPELINE (For training):\")\n",
    "    print(\"  - run_full_pipeline() - Run all 5 phases\")\n",
    "    print(\"  - run_phase(1) - Quick verification\")\n",
    "    print(\"  - run_phase(2) - AOT-GAN training\")\n",
    "    print(\"  - run_phase(3) - ICT training\")\n",
    "    print(\"  - run_phase(4) - RePaint inference\")\n",
    "    print(\"  - run_phase(5) - Evaluation\")\n",
    "    print(\"\")\n",
    "    print(\"💡 Ready to use! Go to the next cell to run commands.\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"💡 Make sure you've run the repository setup cell first\")\n",
    "    print(\"💡 The scripts should be in the scripts/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Quick Workflow: Train, Test & Evaluate on Balanced Dataset\n",
    "\n",
    "The cells below provide different workflows based on your needs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 OPTION 1: Quick Test on Balanced Dataset (Recommended First Step)\n",
    "# Tests all 9 pretrained model variants on the subset_4 (4 balanced test images)\n",
    "# ⏱️ Time: ~30-60 minutes\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 QUICK TEST: All Models on Balanced subset_4\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing 9 model variants on 4 balanced images:\")\n",
    "print(\"  • 2 low BMD images\")\n",
    "print(\"  • 2 high BMD images\")\n",
    "print(\"\")\n",
    "\n",
    "tester = ModelTester(timeout_per_model=600, verbose=True)\n",
    "results = tester.run_comprehensive_test(models=[\"all\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 QUICK TEST COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Successful: {results['summary']['successful']}\")\n",
    "print(f\"❌ Failed: {results['summary']['failed']}\")\n",
    "print(f\"⏭️ Skipped: {results['summary']['skipped']}\")\n",
    "print(\"\")\n",
    "print(\"📁 Check results/ directory for output images\")\n",
    "print(\"📊 Check results/comprehensive_test_results.json for detailed results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎓 OPTION 2: Train on Full Balanced Dataset & Evaluate\n",
    "# Trains models on the full 431-image training set with balanced BMD representation\n",
    "# ⏱️ Time: 6-8 hours (runs all training phases)\n",
    "#\n",
    "# This will:\n",
    "# 1. Train AOT-GAN on 431 balanced training images\n",
    "# 2. Train ICT (Transformer) on 431 balanced training images\n",
    "# 3. Run RePaint inference on test set\n",
    "# 4. Evaluate all models on the 55-image balanced test set\n",
    "# 5. Generate comprehensive metrics and visualizations\n",
    "\n",
    "# Uncomment to run full training pipeline\n",
    "# print(\"=\" * 60)\n",
    "# print(\"🎓 FULL TRAINING PIPELINE ON BALANCED DATASET\")\n",
    "# print(\"=\" * 60)\n",
    "# print(\"Training on:\")\n",
    "# print(\"  • 431 images (215 low BMD + 216 high BMD)\")\n",
    "# print(\"  • 53 validation images (26 low BMD + 27 high BMD)\")\n",
    "# print(\"  • 55 test images (28 low BMD + 27 high BMD)\")\n",
    "# print(\"\")\n",
    "#\n",
    "# run_full_pipeline(timeout_hours=8)\n",
    "\n",
    "print(\"💡 To run full training pipeline:\")\n",
    "print(\"   1. Uncomment the code above\")\n",
    "print(\"   2. Run this cell\")\n",
    "print(\"   3. Wait 6-8 hours for training to complete\")\n",
    "print(\"\")\n",
    "print(\"📊 Benefits of the new balanced split:\")\n",
    "print(\"   • 2x more training data (431 vs ~216)\")\n",
    "print(\"   • Equal representation of low/high BMD cases\")\n",
    "print(\"   • Better model generalization\")\n",
    "print(\"   • More reliable evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 OPTION 3: Test Specific Model on Full Balanced Test Set\n",
    "# Test a single model on all 55 balanced test images (instead of just subset_4)\n",
    "# ⏱️ Time: ~5-15 minutes per model\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def test_model_on_full_test_set(model_name, config_path):\n",
    "    \"\"\"Test a specific model on the full 55-image balanced test set.\"\"\"\n",
    "    print(f\"🧪 Testing {model_name} on full balanced test set (55 images)...\")\n",
    "    print(f\"   Using config: {config_path}\")\n",
    "\n",
    "    # Run the appropriate test script based on model\n",
    "    if \"aot-gan\" in model_name.lower():\n",
    "        cmd = [\n",
    "            \"python\",\n",
    "            \"scripts/test.py\",\n",
    "            \"--model\",\n",
    "            \"aot-gan\",\n",
    "            \"--config\",\n",
    "            config_path,\n",
    "        ]\n",
    "    elif \"ict\" in model_name.lower():\n",
    "        cmd = [\"python\", \"scripts/test.py\", \"--model\", \"ict\", \"--config\", config_path]\n",
    "    elif \"repaint\" in model_name.lower():\n",
    "        cmd = [\n",
    "            \"python\",\n",
    "            \"scripts/test.py\",\n",
    "            \"--model\",\n",
    "            \"repaint\",\n",
    "            \"--config\",\n",
    "            config_path,\n",
    "        ]\n",
    "    else:\n",
    "        print(f\"❌ Unknown model: {model_name}\")\n",
    "        return\n",
    "\n",
    "    result = subprocess.run(\n",
    "        cmd, check=False, capture_output=True, text=True, timeout=1800\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ {model_name} testing completed!\")\n",
    "        print(f\"📁 Results saved to results/{model_name}/\")\n",
    "    else:\n",
    "        print(f\"❌ {model_name} testing failed\")\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# test_model_on_full_test_set(\"AOT-GAN-OAI\", \"configs/oai_config.yml\")\n",
    "# test_model_on_full_test_set(\"ICT-OAI\", \"models/ict/Guided_Upsample/subset_4_config.yml\")\n",
    "\n",
    "print(\"💡 To test a specific model on the full balanced test set:\")\n",
    "print(\"   1. Uncomment one of the examples above\")\n",
    "print(\"   2. Adjust the model name and config path as needed\")\n",
    "print(\"   3. Run this cell\")\n",
    "print(\"\")\n",
    "print(\"📊 Full test set: 55 images (28 low BMD + 27 high BMD)\")\n",
    "print(\"   vs. subset_4: 4 images (2 low BMD + 2 high BMD)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 QUICK START - Test ALL Model Variants (RECOMMENDED)\n",
    "# This tests all 9 pretrained models on 4 OAI X-ray images\n",
    "# Perfect for quick validation and comparison!\n",
    "\n",
    "tester = ModelTester(timeout_per_model=600, verbose=True)\n",
    "results = tester.run_comprehensive_test(models=[\"all\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 COMPREHENSIVE TEST COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Successful: {results['summary']['successful']}\")\n",
    "print(f\"❌ Failed: {results['summary']['failed']}\")\n",
    "print(f\"⏭️ Skipped: {results['summary']['skipped']}\")\n",
    "print(\"\")\n",
    "print(\"📁 Check results/ directory for output images\")\n",
    "print(\"📊 Check results/comprehensive_test_results.json for detailed results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 ALTERNATIVE - Test Specific Model Types\n",
    "# If you only want to test specific models:\n",
    "\n",
    "# Test only AOT-GAN variants\n",
    "# tester = ModelTester(timeout_per_model=600)\n",
    "# results = tester.run_comprehensive_test(models=[\"aot-gan\"])\n",
    "\n",
    "# Test only ICT variants\n",
    "# tester = ModelTester(timeout_per_model=600)\n",
    "# results = tester.run_comprehensive_test(models=[\"ict\"])\n",
    "\n",
    "# Test only RePaint variants\n",
    "# tester = ModelTester(timeout_per_model=600)\n",
    "# results = tester.run_comprehensive_test(models=[\"repaint\"])\n",
    "\n",
    "# Test multiple specific models\n",
    "# tester = ModelTester(timeout_per_model=600)\n",
    "# results = tester.run_comprehensive_test(models=[\"aot-gan\", \"repaint\"])\n",
    "\n",
    "print(\"💡 Uncomment the code above to test specific model types\")\n",
    "print(\"💡 Or run the previous cell to test all models at once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Training Pipeline (Optional)\n",
    "\n",
    "If you want to train new models on OAI data instead of just testing pretrained models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎓 TRAINING PIPELINE - Train models on OAI data\n",
    "# Uncomment to run the full training pipeline (6-8 hours)\n",
    "\n",
    "# Run complete pipeline (all phases)\n",
    "# run_full_pipeline(timeout_hours=8)\n",
    "\n",
    "# Or run individual phases:\n",
    "# run_phase(1)  # Quick verification (5 min)\n",
    "# run_phase(2)  # AOT-GAN training (2-4 hours)\n",
    "# run_phase(3)  # ICT training (1-3 hours)\n",
    "# run_phase(4)  # RePaint inference (30 min)\n",
    "# run_phase(5)  # Evaluation (15 min)\n",
    "\n",
    "print(\"📋 Training Pipeline Options:\")\n",
    "print(\"\")\n",
    "print(\"⚡ Quick verification only:\")\n",
    "print(\"   run_phase(1)\")\n",
    "print(\"\")\n",
    "print(\"🔧 Train specific model:\")\n",
    "print(\"   run_phase(2)  # AOT-GAN only\")\n",
    "print(\"   run_phase(3)  # ICT only\")\n",
    "print(\"   run_phase(4)  # RePaint inference only\")\n",
    "print(\"\")\n",
    "print(\"🚀 Train all models:\")\n",
    "print(\"   run_full_pipeline(timeout_hours=8)\")\n",
    "print(\"\")\n",
    "print(\"⚠️  Warning: Training takes 6-8 hours total\")\n",
    "print(\"💡 Tip: Test with run_phase(1) first to verify setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎨 Comparison Strips Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 GENERATE COMPARISON STRIPS\n",
    "# Creates horizontal comparison images: GT, GT+Mask, and all model outputs\n",
    "# Perfect for visual assessment and thesis inclusion!\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"scripts\")\n",
    "\n",
    "try:\n",
    "    from generate_comparison_strips import main as generate_strips\n",
    "\n",
    "    print(\"🎨 Generating visual comparison strips...\")\n",
    "    print(\"This creates horizontal strips showing all model outputs side-by-side\")\n",
    "    print(\"\")\n",
    "\n",
    "    strip_paths = generate_strips()\n",
    "\n",
    "    if strip_paths:\n",
    "        print(f\"\\n✅ Generated {len(strip_paths)} comparison strips!\")\n",
    "        print(\"📁 Location: results/comparison_strips/\")\n",
    "        print(\"\")\n",
    "        print(\"📸 Files created:\")\n",
    "        for path in strip_paths:\n",
    "            print(f\"  - {path.name}\")\n",
    "        print(\"  - all_comparisons_summary.png (all strips stacked)\")\n",
    "        print(\"\")\n",
    "        print(\"💡 Each strip shows:\")\n",
    "        print(\"   GT → GT+Mask → AOT-GAN variants → ICT variants → RePaint variants\")\n",
    "    else:\n",
    "        print(\"⚠️  No strips generated - check that comprehensive test completed\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"💡 Make sure you've run the setup cells first\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating strips: {e}\")\n",
    "    print(\"💡 Ensure comprehensive test completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Results Visualization and Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualize Results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Check if results exist\n",
    "results_json = Path(\"results/comprehensive_test_results.json\")\n",
    "if results_json.exists():\n",
    "    with results_json.open() as f:\n",
    "        test_results = json.load(f)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 TEST RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Timestamp: {test_results['timestamp']}\")\n",
    "    print(f\"Duration: {test_results['duration']}\")\n",
    "    print(f\"Total tests: {test_results['summary']['total']}\")\n",
    "    print(f\"Successful: {test_results['summary']['successful']}\")\n",
    "    print(f\"Failed: {test_results['summary']['failed']}\")\n",
    "    print(f\"Skipped: {test_results['summary']['skipped']}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Show which models passed\n",
    "    print(\"✅ SUCCESSFUL MODELS:\")\n",
    "    for result in test_results[\"results\"]:\n",
    "        if result[\"success\"]:\n",
    "            print(f\"  • {result['model']} ({result['elapsed']:.1f}s)\")\n",
    "\n",
    "    # Show which models were skipped/failed\n",
    "    if test_results[\"summary\"][\"skipped\"] > 0:\n",
    "        print(\"\")\n",
    "        print(\"⏭️ SKIPPED MODELS:\")\n",
    "        for result in test_results[\"results\"]:\n",
    "            if \"reason\" in result:\n",
    "                print(f\"  • {result['model']} ({result['reason']})\")\n",
    "\n",
    "    if test_results[\"summary\"][\"failed\"] > 0:\n",
    "        print(\"\")\n",
    "        print(\"❌ FAILED MODELS:\")\n",
    "        for result in test_results[\"results\"]:\n",
    "            if not result[\"success\"] and \"reason\" not in result:\n",
    "                print(f\"  • {result['model']}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  No test results found. Run the comprehensive test first!\")\n",
    "    print(\"💡 Execute the 'QUICK START' cell above to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📸 Display Sample Results (if available)\n",
    "# Show inpainted images from successful tests\n",
    "\n",
    "results_base = Path(\"results\")\n",
    "\n",
    "# Find the first successful result directory\n",
    "sample_dirs = []\n",
    "for model_type in [\"AOT-GAN\", \"ICT\", \"RePaint\"]:\n",
    "    model_path = results_base / model_type\n",
    "    if model_path.exists():\n",
    "        for variant in model_path.iterdir():\n",
    "            subset_4_path = variant / \"subset_4\"\n",
    "            if subset_4_path.exists():\n",
    "                # Check for images\n",
    "                image_files = list(subset_4_path.rglob(\"*.png\"))\n",
    "                if image_files:\n",
    "                    sample_dirs.append((f\"{model_type} {variant.name}\", subset_4_path))\n",
    "\n",
    "if sample_dirs:\n",
    "    print(f\"📸 Found results from {len(sample_dirs)} model variants\")\n",
    "    print(\"\")\n",
    "    print(\"Available results:\")\n",
    "    for i, (name, path) in enumerate(sample_dirs):\n",
    "        image_count = len(list(path.rglob(\"*.png\")))\n",
    "        print(f\"  {i + 1}. {name} ({image_count} images)\")\n",
    "\n",
    "    # Display sample from first available model\n",
    "    if len(sample_dirs) > 0:\n",
    "        model_name, sample_path = sample_dirs[0]\n",
    "        sample_images = sorted(sample_path.rglob(\"*.png\"))[:4]\n",
    "\n",
    "        if sample_images:\n",
    "            print(f\"\\n🖼️  Displaying samples from: {model_name}\")\n",
    "            print(f\"📁 Path: {sample_path}\")\n",
    "\n",
    "            # Create grid\n",
    "            fig, axes = plt.subplots(1, min(4, len(sample_images)), figsize=(15, 4))\n",
    "            if len(sample_images) == 1:\n",
    "                axes = [axes]\n",
    "\n",
    "            for idx, img_path in enumerate(sample_images[:4]):\n",
    "                img = Image.open(img_path)\n",
    "                if idx < len(axes):\n",
    "                    axes[idx].imshow(img, cmap=\"gray\")\n",
    "                    axes[idx].set_title(img_path.name)\n",
    "                    axes[idx].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"💡 To view more results, explore the results/ directory\")\n",
    "else:\n",
    "    print(\"⚠️  No result images found yet\")\n",
    "    print(\"💡 Run the comprehensive test first to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Download Results\n",
    "# Package and download all results as a ZIP file\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    results_dir = Path(\"results\")\n",
    "\n",
    "    if results_dir.exists() and any(results_dir.iterdir()):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        zip_filename = f\"oai_inpainting_results_{timestamp}.zip\"\n",
    "\n",
    "        print(f\"📦 Creating results archive: {zip_filename}\")\n",
    "        print(\"This may take a few minutes...\")\n",
    "\n",
    "        # Create ZIP file\n",
    "        with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for file_path in results_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = file_path.relative_to(results_dir.parent)\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    if file_path.suffix in [\".png\", \".jpg\", \".json\", \".txt\"]:\n",
    "                        print(f\"  Added: {arcname}\")\n",
    "\n",
    "        print(f\"✅ Archive created: {zip_filename}\")\n",
    "        print(f\"📦 Size: {Path(zip_filename).stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "        print(\"\")\n",
    "        print(\"⬇️  Downloading...\")\n",
    "\n",
    "        # Download the file\n",
    "        files.download(zip_filename)\n",
    "\n",
    "        print(\"✅ Download initiated!\")\n",
    "        print(\"💡 Check your browser's download folder\")\n",
    "    else:\n",
    "        print(\"⚠️  No results found to download\")\n",
    "        print(\"💡 Run the comprehensive test first to generate results\")\n",
    "else:\n",
    "    print(\"⚠️  Not running in Google Colab\")\n",
    "    print(\"💡 This cell is designed for Colab environment\")\n",
    "    print(\"📁 Results are available locally in the results/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update repository to latest version\n",
    "print(\"🔄 Updating repository...\")\n",
    "\n",
    "try:\n",
    "    # Fetch latest changes\n",
    "    subprocess.run([\"git\", \"fetch\"], check=True)\n",
    "\n",
    "    # Check for updates\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"status\", \"-uno\"], check=False, capture_output=True, text=True\n",
    "    )\n",
    "    print(\"📋 Repository status:\")\n",
    "    print(result.stdout)\n",
    "\n",
    "    # Pull latest changes\n",
    "    subprocess.run([\"git\", \"pull\"], check=True)\n",
    "    print(\"✅ Repository updated to latest version\")\n",
    "\n",
    "    # Reinstall dependencies if needed\n",
    "    print(\"📦 Reinstalling dependencies...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-e\", \".[dev,ml]\"], check=True)\n",
    "    print(\"✅ Dependencies updated\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Update failed: {e}\")\n",
    "    print(\"💡 You may need to restart the runtime and re-run the setup cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting and Tips\n",
    "print(\"🔧 Troubleshooting and Tips\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n📋 Common Issues and Solutions:\")\n",
    "print(\"1. ❌ 'run_phase' is not defined\")\n",
    "print(\"   💡 Solution: Run the 'Pipeline Runner Setup' cell first\")\n",
    "print(\"\")\n",
    "print(\"2. ❌ Import errors\")\n",
    "print(\"   💡 Solution: Restart runtime and re-run all setup cells\")\n",
    "print(\"\")\n",
    "print(\"3. ❌ Data not found\")\n",
    "print(\"   💡 Solution: Check Google Drive mounting and data structure\")\n",
    "print(\"\")\n",
    "print(\"4. ❌ GPU not available\")\n",
    "print(\"   💡 Solution: Enable GPU in Runtime → Change runtime type\")\n",
    "print(\"\")\n",
    "print(\"5. ❌ Out of memory\")\n",
    "print(\"   💡 Solution: Reduce batch size or restart runtime\")\n",
    "\n",
    "print(\"\\n🚀 Quick Commands:\")\n",
    "print(\"• Check GPU: !nvidia-smi\")\n",
    "print(\"• Check directory: !pwd\")\n",
    "print(\"• List files: !ls -la\")\n",
    "print(\"• Check Python: !python --version\")\n",
    "print(\"• Check PyTorch: !python -c 'import torch; print(torch.__version__)'\")\n",
    "\n",
    "print(\"\\n📞 Getting Help:\")\n",
    "print(\"• Check the README.md for detailed instructions\")\n",
    "print(\"• Review error messages carefully\")\n",
    "print(\"• Restart runtime if issues persist\")\n",
    "print(\"• Ensure all setup cells have run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
